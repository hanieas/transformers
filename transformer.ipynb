{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "175d7a99-3744-4c97-8ac0-0e6e867a6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3507e548-c5bd-4bd4-9dab-443a85f0bc8a",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9219754a-f411-4b62-b9e2-08079cdf297d",
   "metadata": {},
   "source": [
    "<img src=\"./transformer.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f151eb-4a52-4466-a43a-3e4af69ead24",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653efdc8-afaa-4419-91f3-54d95bea3afd",
   "metadata": {},
   "source": [
    "Embedding vectors will create a more semantic representation of each word.\n",
    "Suppoese each embedding vector is of 512 dimension and suppose our vocab size is 100, then our embedding matrix will be of size 100x512. These marix will be learned on training and during inference each word will be mapped to corresponding 512 d vector. Suppose we have batch size of 32 and sequence length of 10(10 words). The the output will be 32x10x512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01af3b04-9cd0-4dca-9838-b2a6eaea51d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: size of the vocabulary\n",
    "            emebed_dim: dimension of embedding\n",
    "        \"\"\"\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            out: embedding vector\n",
    "        \"\"\"\n",
    "        out = self.embedding(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615821d0-84ab-406c-bcce-7694f4371ea5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeae94c-b8fe-49b7-b72b-1c26c88be660",
   "metadata": {},
   "source": [
    "Next step is to generate positional encoding. Inorder for the model to make sense of the sentence, it needs to know two things about the each word.\n",
    "\n",
    "    1. what does the word mean?\n",
    "    2. what is the position of the word in the sentence.\n",
    "    \n",
    "In \"attention is all you need paper\" author used the following functions to create positional encoding. On odd time steps a cosine function is used and in even time steps a sine function is used.\n",
    "\n",
    "<img src=\"./positional_embedding.png\" style=\"width:350px; height:110px\"/>\n",
    "\n",
    "Positinal embedding will generate a matrix of similar to embedding matrix. It will create a matrix of dimension sequence length x embedding dimension. For each token(word) in sequence, we will find the embedding vector which is of dimension 1 x 512 and it is added with the correspondng positional vector which is of dimension 1 x 512 to get 1 x 512 dim out for each word/token.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./embedding_layer.png\" style=\"width:350px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e115ccf0-ea24-422d-a8a6-80aee26342d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_sequence_len, embed_dim):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            max_sequence_len: length of input sentence\n",
    "            embed_dim: dimension of embedding vector\n",
    "        \"\"\"\n",
    "        super(PositionalEmbedding,self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "        \n",
    "        pe = torch.zeros(max_sequence_len, self.embed_dim)\n",
    "\n",
    "        for pos in range(max_sequence_len):\n",
    "            for i in range(0, self.embed_dim, 2):\n",
    "                pe[pos, i] = math.sin(pos / 1000 ** ((2 * i) / self.embed_dim))\n",
    "                pe[pos, i+1] = math.cos(pos / 1000 ** ( (2 * (i+1)) / self.embed_dim ))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "        # If you have parameters in your model, which should be saved and restored in the state_dict,\n",
    "        # but not trained by the optimizer, you should register them as buffers.\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            x: output vector\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc881121-7d18-40b9-95b4-f508e176f14b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd7245d2-233b-44d3-8a7c-eb5ee0a33c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, n):\n",
    "    \"\"\" Products n identical layers\n",
    "    Args:\n",
    "        module: The layer\n",
    "        n: count of layers\n",
    "    Returns\n",
    "        list of modules\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdd45244-1394-4f25-8997-b3bd1c49901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_fn(query, key, value, mask=None):\n",
    "    \"\"\" compute the scaled dot product attention\n",
    "    Args:\n",
    "        query: query vector\n",
    "        key: key vector\n",
    "        value: value vector\n",
    "    \"\"\"\n",
    "    single_head_dim = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2,-1)) / math.sqrt(single_head_dim)\n",
    "   \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    \n",
    "    return torch.matmul(p_attn, value), p_attn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bd9da48-e676-4c31-8c9f-adbfcaef8478",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=512, n_heads=8):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            embed_dim: dimension of embedding vector\n",
    "            n_heads: number of heads in multi-head attention\n",
    "        \"\"\"\n",
    "        \n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim #512\n",
    "        self.n_heads = n_heads     #8\n",
    "        self.single_head_dim = int(self.embed_dim / self.n_heads) # 512/8 = 64 each query,key,value vector will be of 64d\n",
    "        self.linears = clones(nn.Linear(self.embed_dim, self.embed_dim), 4)\n",
    "        self.attn = None\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "            \n",
    "        n_batches = query.size(0)\n",
    "        # print(\"query shape in multihead attention\", query.shape)\n",
    "            \n",
    "        query, key, value = [\n",
    "            lin(x).view(n_batches, -1, self.n_heads, self.single_head_dim).transpose(1,2)\n",
    "            for lin, x in zip(self.linears,(query, key, value))\n",
    "        ]\n",
    "        \n",
    "        x, self.attn = attention_fn(query, key, value, mask)\n",
    "        \n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(n_batches, -1, self.n_heads * self.single_head_dim)\n",
    "        )\n",
    "        \n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e4c162-9dc3-4711-a4cb-70befc1fae95",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e94924d-b60a-485f-b8f6-53c7e0ad82c2",
   "metadata": {},
   "source": [
    "<img src=\"./encoder.png\" style=\"height:450px; width:280px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a03bd42a-1dac-4e69-bcbe-746684fa20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor, n_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           embed_dim: dimension of the embedding\n",
    "           expansion_factor: factor which determines output dimension of linear layer\n",
    "           n_heads: number of attention heads\n",
    "        \"\"\"\n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, expansion_factor*embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(expansion_factor*embed_dim,embed_dim)\n",
    "        )\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, query, key, value):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           key: key vector\n",
    "           query: query vector\n",
    "           value: value vector\n",
    "           norm2_out: output of transformer block\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        attention_out = self.attention(key,query,value)  #32x10x512\n",
    "        attention_residual_out = attention_out + value  #32x10x512\n",
    "        norm1_out = self.dropout1(self.norm1(attention_residual_out)) #32x10x512\n",
    "\n",
    "        feed_fwd_out = self.feed_forward(norm1_out) #32x10x512 -> #32x10x2048 -> 32x10x512\n",
    "        feed_fwd_residual_out = feed_fwd_out + norm1_out #32x10x512\n",
    "        norm2_out = self.dropout2(self.norm2(feed_fwd_residual_out)) #32x10x512\n",
    "\n",
    "        return norm2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "146f1cd1-bd43-47fd-b3a7-e410290ca1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        seq_len : length of input sequence\n",
    "        embed_dim: dimension of embedding\n",
    "        num_layers: number of encoder layers\n",
    "        expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "        n_heads: number of heads in multihead attention\n",
    "        \n",
    "    Returns:\n",
    "        out: output of the encoder\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seq_len, vocab_size, embed_dim, num_layers=2, expansion_factor=4, n_heads=8, ):\n",
    "        super(TransformerEncoder,self).__init__()\n",
    "    \n",
    "        self.embedding_layer = Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encode = PositionalEmbedding(seq_len, embed_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([TransformerBlock(embed_dim, expansion_factor, n_heads) for i in range(num_layers)])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        embedding_out = self.embedding_layer(x)\n",
    "        positional_embed_out = self.positional_encode(embedding_out)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            encoder_out = layer(positional_embed_out,positional_embed_out,positional_embed_out)\n",
    "\n",
    "        return encoder_out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9810cd0-ab96-4e76-aef6-9faf32e90fee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c23d2a-85af-4bc4-8816-d1e22df8ee05",
   "metadata": {},
   "source": [
    "<img src=\"./decoder.webp\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6047b7f3-717f-43eb-b05b-f2c8f5ea321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,embed_dim, expansion_factor=4, n_heads=8):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           key: key vector\n",
    "           query: query vector\n",
    "           value: value vector\n",
    "           mask: mask to be given for multi head attention \n",
    "        Returns:\n",
    "           out: output of transformer block\n",
    "    \n",
    "        \"\"\"\n",
    "        \n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, expansion_factor, n_heads)\n",
    "        \n",
    "    def forward(self, key, query, x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           key: key vector\n",
    "           query: query vector\n",
    "           value: value vector\n",
    "           mask: mask to be given for multi head attention \n",
    "        Returns:\n",
    "           output: output of transformer block\n",
    "    \n",
    "        \"\"\"\n",
    "        attention_out = self.attention(x, x, x, mask)\n",
    "        print('here')\n",
    "        value = self.dropout( self.norm( attention_out + x ) )\n",
    "        output = self.transformer_block(key, query, value)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2884852-53a2-4858-b080-5b7279a508e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,target_vocab_size, embed_dim, seq_len, num_layers=2, expansion_factor=4, n_heads =8):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \"\"\"  \n",
    "        Args:\n",
    "           target_vocab_size: vocabulary size of taget\n",
    "           embed_dim: dimension of embedding\n",
    "           seq_len : length of input sequence\n",
    "           num_layers: number of encoder layers\n",
    "           expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "           n_heads: number of heads in multihead attention\n",
    "        \n",
    "        \"\"\"\n",
    "        self.embedding_layer = Embedding(target_vocab_size, embed_dim)\n",
    "        self.positional_encode = PositionalEmbedding(seq_len, embed_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(embed_dim, expansion_factor, n_heads)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.fc_layer = nn.Linear(embed_dim, target_vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, encoder_out, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector from target\n",
    "            encoder_out : output from encoder layer\n",
    "            trg_mask: mask for decoder self attention\n",
    "        Returns:\n",
    "            output: output vector\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.positional_encode(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(encoder_out, x, encoder_out, mask)\n",
    "        \n",
    "        output = F.softmax(self.fc_layer(x))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc47d91-8d42-412c-b7e5-c9dfe3429e03",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7aa0f82d-271d-447e-a855-8ff16bf1fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_dim, src_vocab_size, target_vocab_size, seq_len, num_layers=2, expansion_factor=4, n_heads=8):\n",
    "        super(Transformer,self).__init__()\n",
    "        \n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.encoder = TransformerEncoder(seq_len,src_vocab_size, embed_dim, num_layers,expansion_factor, n_heads)\n",
    "        self.decoder = TransformerDecoder(target_vocab_size, embed_dim,seq_len, num_layers,expansion_factor, n_heads)\n",
    "        \n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trg: target sequence\n",
    "        Returns:\n",
    "            trg_mask: target mask\n",
    "        \"\"\"\n",
    "        n_batches, trg_len = trg.shape\n",
    "        \n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            n_batches,trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask    \n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: input to encoder \n",
    "            trg: input to decoder\n",
    "        out:\n",
    "            out: final vector which returns probabilities of each target word\n",
    "        \"\"\"\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src)\n",
    "        print(src.shape)\n",
    "\n",
    "        outputs = self.decoder(trg, enc_out, trg_mask)\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b1cd7-69b9-4f26-96fe-94bf16991e60",
   "metadata": {},
   "source": [
    "## Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa4268b4-f371-4fb1-9f16-4a19cef7d2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 12]) torch.Size([32, 12])\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = 1000\n",
    "target_vocab_size = 1000\n",
    "num_layers = 6\n",
    "seq_len= 12\n",
    "n_batches = 32\n",
    "\n",
    "\n",
    "# let 0 be sos token and 1 be eos token\n",
    "src = torch.randint(src_vocab_size, (n_batches, seq_len))\n",
    "target = torch.randint(target_vocab_size, (n_batches, seq_len))\n",
    "\n",
    "print(src.shape,target.shape)\n",
    "model = Transformer(embed_dim=512, src_vocab_size=src_vocab_size, \n",
    "                    target_vocab_size=target_vocab_size, seq_len=seq_len,\n",
    "                    num_layers=num_layers, expansion_factor=4, n_heads=8)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d80a9e7c-0776-4468-84a9-719651f5b024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p attention torch.Size([32, 8, 12, 12])\n",
      "value torch.Size([32, 8, 12, 64])\n",
      "p attention torch.Size([32, 8, 12, 12])\n",
      "value torch.Size([32, 8, 12, 64])\n",
      "p attention torch.Size([32, 8, 12, 12])\n",
      "value torch.Size([32, 8, 12, 64])\n",
      "p attention torch.Size([32, 8, 12, 12])\n",
      "value torch.Size([32, 8, 12, 64])\n",
      "p attention torch.Size([32, 8, 12, 12])\n",
      "value torch.Size([32, 8, 12, 64])\n",
      "p attention torch.Size([32, 8, 12, 12])\n",
      "value torch.Size([32, 8, 12, 64])\n",
      "p attention torch.Size([32, 8, 12, 12])\n",
      "value torch.Size([32, 8, 12, 64])\n",
      "p attention torch.Size([32, 8, 12, 12])\n",
      "value torch.Size([32, 8, 12, 64])\n",
      "p attention torch.Size([32, 8, 12, 12])\n",
      "value torch.Size([32, 8, 12, 64])\n",
      "p attention torch.Size([32, 8, 12, 12])\n",
      "value torch.Size([32, 8, 12, 64])\n",
      "p attention torch.Size([32, 8, 12, 12])\n",
      "value torch.Size([32, 8, 12, 64])\n",
      "p attention torch.Size([32, 8, 12, 12])\n",
      "value torch.Size([32, 8, 12, 64])\n",
      "p attention torch.Size([32, 8, 12, 12])\n",
      "value torch.Size([32, 8, 12, 64])\n",
      "p attention torch.Size([32, 8, 12, 12])\n",
      "value torch.Size([32, 8, 12, 64])\n",
      "p attention torch.Size([32, 8, 12, 12])\n",
      "value torch.Size([32, 8, 12, 64])\n",
      "p attention torch.Size([32, 8, 12, 12])\n",
      "value torch.Size([32, 8, 12, 64])\n",
      "p attention torch.Size([32, 8, 12, 12])\n",
      "value torch.Size([32, 8, 12, 64])\n",
      "p attention torch.Size([32, 8, 12, 12])\n",
      "value torch.Size([32, 8, 12, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pb/4t256kf1739f02pn7ldh71lw0000gn/T/ipykernel_13251/531710156.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(self.fc_layer(x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 12, 1000])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(src, target)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "id": "87a6f860-445a-41e0-8b90-ae01068e4697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(self, src, trg):\n",
    "        \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        src: input to encoder \n",
    "        trg: input to decoder\n",
    "    out:\n",
    "        out_labels : returns final prediction of sequence\n",
    "    \"\"\"\n",
    "    trg_mask = make_trg_mask(trg)\n",
    "    encoder_output = self.encoder(src)\n",
    "    out_labels = []\n",
    "    n_batches, seq_len = src.shape[0], src.shape[1]\n",
    "    output = trg\n",
    "    \n",
    "    for i in range(seq_len):\n",
    "        output = self.decoder(output, encoder_output, trg_mask)\n",
    "        output = output[:,-1,:]\n",
    "        \n",
    "        output = output.argmax(-1)\n",
    "        out_labels.append(output.item())\n",
    "        output = torch.unsqueeze(output,axis=0)\n",
    "        \n",
    "        return out_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494ceec2-3fb3-4dbc-b17b-fc76efeb30b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
